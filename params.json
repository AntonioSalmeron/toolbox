{"name":"AMIDST Toolbox 1.0","tagline":"A Java Library for Analysis of MassIve Data Streams using Probabilistic Graphical Models","body":"# Scope\r\n\r\nThis toolbox offers a collection of scalable and parallel algorithms for inference and learning of hybrid Bayesian networks from streaming data. For example, AMIDST provides parallel multi-core implementations for Bayesian parameter learning, using streaming variational Bayes and variational message passing. Additionally, AMIDST efficiently leverages existing functionalities and algorithms by interfacing to existing software tools such as [R](https://www.r-project.org/), [Hugin](http://www.hugin.com) and [MOA](http://moa.cms.waikato.ac.nz). AMIDST is an open source toolbox written in Java and is available under the Apache Software License 2.0.\r\n\r\nIn the next figure we show a taxonomy of relevant data mining tools dealing with PGMs and data streams. To the best of our knowledge, there is no other software for mining data streams based on PGMs, most of the existing softwares based on PGMs are only focused on mining stationary data sets. Hence, the main goal of AMIDST is to fill this gap and produce a significant contribution within the areas of PGMs and mining streaming data.\r\n\r\n<p align=\"center\">\r\n<img title=\"Taxonomy\" src=\"https://github.com/amidst/toolbox/blob/master/doc/Taxonomy.png?raw=true\" width=\"400\">\r\n</p>\r\n\r\n\r\n# Scalability\r\n\r\nScalability is a main concern for the AMIDST toolbox. As mentioned before, we exploit Java 8 functional programming style to provide parallel implementations of most of our algorithms. If more computation capacity is needed to process data streams, AMIDST users can also use more CPU cores. As an example, the following figure shows how the data processing capacity of our toolbox increases with the number of cores when learning a hybrid BN model with latent variables using the AMIDST's learning engine. More precisely we learn a PGM model with multinomial (blue nodes) and Gaussian (green nodes) variables, some of them are latent, non observable, variables (dashed nodes). As can be seen, using our variational learning engine AMIDST toolbox is able to process data in the order of gigabytes per hour depending on the number of available cores with large and complex PGMs with latent variables.\r\n\r\n<p align=\"center\">\r\n<img src=\"https://github.com/amidst/toolbox/blob/master/doc/Scalability.png?raw=true\" width=\"800\">\r\n</p>\r\n\r\n\r\n# Documentation<a name=\"documentation\"></a>\r\n\r\n1. [Toolbox Functionalities](#functionalities)\r\n   * [Data Streams](#datastreams)\r\n   * [Probabilistic Graphical Models](#pgms)\r\n   * [Inference Engine](#inference)\r\n   * [Learning Engine](#learning)\r\n   * [Concept Drift](#conceptdrift)\r\n   * [Links to MOA, Hugin and R](#librarylinks)\r\n2. [Toolbox Architecture](#architecture)\r\n   * [Description](#description)\r\n   * [Java 8 Integration: Lambdas, streams, and functional-sytle programming](#java8)\r\n   * [Installing AMIDST Toolbox](#installation)\r\n       * [Installing MOALink](#installmoa)\r\n       * [Installing HuginLink](#installhugin)\r\n   * [Compiling & Running from the command line](#compilation)\r\n3. [Contributing to AMIDST](#extension)\r\n4. [Code Examples](#examples)\r\n   * [Data Streams](#datastreamsexample)\r\n   * [Random Variables](#variablesexample)\r\n   * [Bayesian Networks](#bnexample)\r\n       * [Creating Bayesian Networks](#bnnohiddenexample)\r\n       * [Creating Bayesian Networks with latent variables](#bnhiddenexample)\r\n       * [Modifying Bayesian Networks](#bnmodifyexample)\r\n   * [I/O Functionality](#ioexample)\r\n       * [I/O of Data Streams](#iodatastreamsexample)\r\n       * [I/O of Bayesian Networks](#iobnsexample)\r\n   * [Inference Algorithms](#inferenceexample)\r\n       * [The Inference Engine](#inferenceengingeexample)\r\n       * [Variational Message Passing](#vmpexample)\r\n       * [Importance Sampling](#isexample)\r\n   * [Learning Algorithms](#learningexample)\r\n       * [Maximum Likelihood](#mlexample)\r\n       * [Parallel Maximum Likelihood](#pmlexample)\r\n       * [Streaming Variational Bayes](#svbexample)\r\n       * [Parallel Streaming Variational Bayes](#psvbexample)\r\n   * [Concept Drift Methods](#conceptdriftexample)\r\n       * [Maximum Likelihood with Fading](#mlfadingexample)\r\n       * [Streaming Variational Bayes with Fading](#svbfadingexample)\r\n       * [Naive Bayes with Virtual Concept Drift Detection](#nbconceptdriftexample)\r\n   * [HuginLink](#huginglinkexample)\r\n       * [Models conversion between AMIDST and Hugin](#huginglinkconversionexample)\r\n       * [I/O of Bayesian Networks with Hugin net format](#huginglinkioexample)\r\n       * [Invoking Hugin's inference engine](#huginglinkinferenceexample)\r\n       * [Invoking Hugin's Parallel TAN](#huginglinkTANexample)\r\n   * [MoaLink](#moalinkexample)\r\n       * [AMIDST Classifiers from MOA](#moalinkclassifiersexample)\r\n       * [AMIDST Regression from MOA](#moalinkregressionsexample)\r\n5. [API Java Doc](http://amidst.github.io/toolbox/javadoc/index.html)\r\n6. [Citing AMIDST Toolbox](#cite)\r\n\r\n## Toolbox Functionalities<a name=\"functionalities\"></a>\r\n\r\nThe AMIDST is an open source Java 8 toolbox that makes use of functional programming style to provide parallel processing on mutli-core CPUs \\citep{CIM2015}. AMIDST provides a collection of functionalities and algorithms for learning hybrid Bayesian networks from streaming data. In what follows, we describe the main functionalities that AMIDST toolbox supplies.\r\n\r\n[[Back to Top]](#documentation)\r\n\r\n###Data Streams<a name=\"datastreams\"></a> \r\nAMIDST provides parallel processing built-in functionalities for dealing with streaming data \\citep{CIM2015}. It is possible to make several passes over the data samples if the stream can be stored on disk, otherwise the samples are discarded after being processed. The data format supported by AMIDST is Weka's ARFF (Attribute-Relation File Format) \\citep{Hall2009}.\r\n\r\n[[Back to Top]](#documentation)\r\n\r\n###Probabilistic Graphical Models<a name=\"pgms\"></a>\r\nAMIDST currently includes efficient implementations for representing Bayesian networks. AMIDST supports both discrete and continuous variables, and besides Multionomial, Gaussian and conditional linear Gaussian distributions, it also supports other distributions such as Gamma, Poission, Dirichlet, etc. as far as the final BN can be represented as a \\textit{conjugate-exponential family model} \\citep{WinnBishop2005}.  Other kind of probabilistic graphical models, such as dynamic BNs, are expected to be included in this toolbox.\r\n\r\n[[Back to Top]](#documentation)\r\n\r\n###Inference Engine<a name=\"inference\"></a>\r\nAMIDST includes the implementation of the \\textit{variational message passing} \\citep{WinnBishop2005} algorithm, and the parallel implementation of the \\textit{importance sampling} \\citep{hammersley1964monte,CAEPIA2015} algorithm. It also supports exact inference by interfacing with [Hugin](http://www.hugin.com)'s junction tree inference algorithm \\citep{Madsen2005Hugin}. \r\n\r\n[[Back to Top]](#documentation)\r\n\r\n###Learning Engine<a name=\"learning\"></a>  \r\nIn AMIDST, a fully Bayesian approach is pursued, which means that the parameter learning reduces to the task of inference. AMIDST provides a multi-core parallel implementation of the \\textit{streaming variational Bayes} algorithm \\citep{broderick2013streaming}, using \\textit{variational message passing} as underlying inference engine, which can deal with large models with latent variables. When the model does not contain latent variables, an efficient parallel implementation of \\textit{maximum likelihood estimation} \\citep{mlestimation} can be also used by exploiting an efficient vector-based representation of BNs as \\textit{exponential family models} \\citep{WinnBishop2005}. For structural learning, AMIDST currently supports standard PC and parallel TAN algorithms by interfacing with [Hugin](http://www.hugin.com) \\citep{Madsen2005Hugin,Madsen2014}.\r\n\r\n[[Back to Top]](#documentation)\r\n\r\n###Concept drift<a name=\"conceptdrift\"></a> \r\nAMIDST also offers some support for dealing with concept drift while learning BNs from data streams. Firstly, we provide an extension of the \\textit{streaming variational Bayes} algorithm \\citep{broderick2013streaming} which exponentially down-weights the influence of \\textit{old} data samples with the use of a fading factor (TODO). So, models learnt with this approach will be \\textit{focused} in most recent data. In addition, AMIDST provides a probabilistic concept drift detector based on the use of latent variables \\citep{IDA2015}.\r\n\r\n[[Back to Top]](#documentation)\r\n\r\n###Links to MOA, Hugin and R<a name=\"librarylinks\"></a> \r\nAMIDST leverages existing functionalities and algorithms by interfacing to existing software tools such as [R](https://www.r-project.org/), [Hugin](http://www.hugin.com) and [MOA](http://moa.cms.waikato.ac.nz) (Massive Online Analysis) \\citep{BifetHolmesKirkbyPfahringer10}. This allows to efficiently well established systems and also broaden the AMIDST user-base. \r\n\r\n* **HuginLink** consists of a set of functionalities implemented to link the AMIDST toolbox with [Hugin](http://www.hugin.com) commercial software \\citep{Madsen2005Hugin}. This connection extends AMIDST by providing some of the main functionalities of [Hugin](http://www.hugin.com), such as exact inference algorithms and scalable structural learning algorithms \\citep{Madsen2014}. [Hugin](http://www.hugin.com) is a third-party commercial software and to access to these functionalities it is needed a license of the software and to follow some specific installation steps (further information is given [here](http://amidst.github.io/toolbox/#installhugin)).\r\n\r\n* **MoaLink** ensures an easy use of AMDIST functionalities within [MOA](http://moa.cms.waikato.ac.nz) \\citep{BifetHolmesKirkbyPfahringer10}.  The main idea is that any model deployed in AMIDST can be integrated and evaluated using MOA's graphical user interface. As a proof of concept, \\textit{MoaLink} already provides a classification, a regression and a clustering method based on BN models with latent variables. These models are learnt in a streaming fashion using AMIDST learning engine. \r\n\r\n* **RLink** ....\r\n\r\n[[Back to Top]](#documentation)\r\n\r\n## Toolbox Code Architecture<a name=\"architecture\"></a>\r\n\r\n### Description<a name=\"description\"></a>\r\nAMIDST toolbox is an open source project under [Apache Software License 2.0](http://www.apache.org/licenses/LICENSE-2.0). It is written in Java and is based on [Apache Maven](https://en.wikipedia.org/wiki/Apache_Maven) for building and structuring the project. This toolbox is structured as [multi-module Maven project](http://books.sonatype.com/mvnex-book/reference/multimodule.html). Roughly speaking, a **Maven module** is an independent piece of software with explicit dependencies to other modules in the project and to other external libraries. Each module is placed in independent folders and contains an xml file describing its dependencies. In this current version, the toolbox is composed by the following four modules:\r\n\r\n* **Core module** contains all the main functionalities of the toolbox. It is placed in the *core* folder. Go to the [Java Doc](http://amidst.github.io/toolbox/javadoc/index.html) for details about the different Java classes. \r\n\r\n* **Examples module** contains basic code examples showing how to use the main functionalities of the toolbox. It is placed in the *examples* folder under the root project folder.\r\n\r\n* **MoaLink module** contains the code needed to use the AMIDST functionality within MOA. It is placed in the *moalink* folder  under the root project folder.\r\n\r\n* **HuginLink module** contains the code needed to use [Hugin](www.hugin.com) software within AMIDST. It is placed in the *huginlink* folder under the root project folder. \r\n\r\n[[Back to Top]](#documentation)\r\n\r\n### Java 8 Integration: Lambdas, streams, and functional-sytle programming<a name=\"java8\"></a>\r\n\r\nThis toolbox has been specifically designed for using the functional-style features provided by the Java 8 release. This design leverages these new features for developing easy-to-code parallel algorithms on mutli-core CPUs. As commented above, the main scalability properties of this toolbox rely on this functional-style approach introduced in Java 8. Our aim is that future developers can also exploit this specific design of the toolbox for easily developing new methods for dealing with massive data streams using PGMs.  \r\n\r\nOur paper [Probabilistic Graphical Models on Multi-Core CPUs using Java 8]() provides a deep discussion over the different design issues we had to face and how they were solved using Java 8 functional-style features. \r\n\r\n[[Back to Top]](#documentation)\r\n\r\n### Installing AMIDST Toolbox <a name=\"installation\"></a>\r\n\r\nThe first step is to install [Java 8](http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html).  We strongly recommend [IntelliJ](http://www.jetbrains.com/idea/download/) as IDE tool because it has direct support of Maven and Github. \r\n\r\nNow, we detail two different installation settings based on Maven and IntelliJ. The first installation settings is for those who just want to use the AMIDST toolbox and do not plan to make contributions/extensions to this open software project. The second settings details how to proceed to be a contributor of this project. \r\n\r\n* **Using AMIDST** simply requires to create a new **Maven Project** using IntelliJ where your code will be placed. Then edit the file \"pom.xml\" and add the following lines referring to the link the AMIDST jar library inside the dependencies plugin (follow this [link](http://books.sonatype.com/mvnex-book/reference/customizing-sect-add-depend.html) for further details) and then you are ready to rock. \r\n\r\n        <dependency>\r\n            <groupId>eu.amidst.toolbox</groupId>\r\n            <artifactId>AMIDST</artifactId>\r\n            <version>1.0</version>\r\n        </dependency>\r\n \r\n* **Contributing to AMDIST** is based on the [Fork & Pull](https://help.github.com/articles/using-pull-requests/) collaboration model. Read this [guide](https://guides.github.com/activities/forking/) for full details about how to fork a project and make a pull request. Once you have forked the project and make a local copy to you computer, just you can just open with Intellij the project by pointing at the pom file. Further details about how to contribute to this project are given this [section](#extension). \r\n\r\n[[Back to Top]](#documentation)\r\n\r\n#### Installing MOALink<a name=\"installmoa\"></a>\r\n\r\nTo use AMIDST functionality within [MOA](http://moa.cms.waikato.ac.nz) you just have to run the script \\texttt{compileWithDependencies.sh} in the *moalink* directory. A file *moalink-1.0-SNAPSHOT-jar-with-dependencies.jar* will be generated in the \\texttt{moalink/target} directory. Please place this jar file on your library path for [MOA](http://moa.cms.waikato.ac.nz). \r\n\r\nWith this jar file we can make use of the different learning and inference algorithms in AMIDST to learn more expressive Bayesian network models for classification, regression and clustering. AMIDST offers the possibility to add latent Gaussian and/or Multinomial variables to a base naive Bayes structure. Normally, the addition of these latent variables should provide classifiers with lower bias and higher variance, that is, more sophisticated classifiers that are able to lean more complex interdependencies in the data, but also more prone to overfit. The user should evaluate the complexity of his/her dataset and choose the number of latent Gaussian variables and states of the multinomial latent variable accordingly.\r\n\r\nWith the following command, [MOA](http://moa.cms.waikato.ac.nz) gui can be invoked e.g (remember to place \\texttt{compileWithDependencies.sh} under the lib folder reference in the *-cp* option):\r\n\r\n```\r\njava -Xmx512m -cp \"../lib/*\" -javaagent:../lib/sizeofag-1.0.0.jar moa.gui.GUI\r\n```\r\n\r\nNote that the above example should be slightly adapted to run on a Windows machine: e.g. use \\textasciicircum~ instead of \\textbackslash~ to escape brackets.\r\n\r\n#### Installing HuginLink<a name=\"installhugin\"></a>\r\n\r\nHuginLink is present in the toolbox as a independent Maven module. To use this module and, in consequence, access some of the functionalities provided by [Hugin](http://www.hugin.com/) commercial software, we need to perform the following steps:\r\n\r\n1. **Install Hugin software**. Here we we describe how to install [Hugin Lite 8.2](http://www.hugin.com/productsservices/demo) which is a freely available demo version of [Hugin](http://www.hugin.com) software. For those with a full license or who want to update HuginLink to link to a new version of [Hugin](http://www.hugin.com) sofware, just follow the same steps.\r\n<!---\r\n2. **Install the jar file**. Inside the installation folder of [Hugin Lite](http://www.hugin.com/productsservices/demo), you will find a jar file. Copy this file to the folder [project-root-folder]/huginlink/huginlib/. Then, edit the file [project-root-folder]/huginlink/pom.xml to correctly reference to this jar file. The lines to edit are the following:\r\n\r\n        <dependency>\r\n            <groupId>com.hugin</groupId>\r\n            <artifactId>hugin</artifactId>\r\n            <version>8.2</version>\r\n            <scope>system</scope>\r\n            <systemPath>${project.basedir}/huginlib/hapi82-64.jar</systemPath>\r\n        </dependency>\r\n-->\r\n\r\n2. **Install the binary file**. Inside the installation folder you will find a folder called *Libraries* which contains the binary file needed for the installation. Choose the file that fits with your operating system and copy it to the folder [project-root-folder]/huginlink/huginlib/. For example, for a MAC OS X with 64 bits architecture the file needed is *libhapi82-64.jnilib*. Finally, rename this binary the file to match it with the Java jar file already provided by the toolbox, *[project-root-folder]/huginlink/huginlib/hapi82_amidst-64.jar*. The final name of the binary file should be *libhapi82_amidst-64* while the extension should not be modify. \r\n\r\n\r\nNow, you can invoke the following example by using the script *run.sh* :\r\n\r\n    ./run.sh eu.amidst.huginlink.examples.inference.HuginInferenceExample\r\n\r\n\r\nWe notice that ror running any code invoking the Hugin API, you have to provide the following option to the JVM  \r\n\r\n    -Djava.library.path=\"./huginlink/huginlib/\" \r\n\r\n\r\n### Compiling & Running from the command line<a name=\"compilation\"></a>\r\n\r\n1. Install Maven: http://maven.apache.org/download.cgi  (follow specific instructions for your OS).\r\n\r\n2. Modify the file maven_startup.sh (which you can find in the root project folder) and fix the path of your maven (Line 5) and java installation (Line 9).\r\n\r\n3. Create (or modify if already exists) a file \".profile\" or \".bash_profile\" in you home directory and add the following line,\r\nwhich points to file \"maven_startup.sh\"\r\n\r\n        source <project-folder>/maven_startup.sh\r\n\r\n Now after restarting the terminal, mvn should work.\r\n\r\n\r\n4. The script \"compile.sh\" (which you can find in the root project folder) just compiles the whole project.\r\n\r\n\r\n5. The script \"run.sh\" (which you can find in the root project folder) should be used to run some class. For example,\r\n\r\n        ./run.sh eu.amidst.core.examples.learning.ParallelMaximumLikelihoodExample\r\n\r\n[[Back to Top]](#documentation)\r\n\r\n## Contributing to AMIDST <a name=\"extension\"></a>\r\n\r\nDevelopers are expected to contribute to this open software following the [Fork & Pull](https://help.github.com/articles/using-pull-requests/) collaboration model. Read this [guide](https://guides.github.com/activities/forking/) for full details about how to fork a project and make a pull request.\r\n\r\nWe establish the following categorization for the contributions to the toolbox. Each one is associated with a different collaboration schemes which is also detailed below.  \r\n\r\n* (A) **Basic Contributions** encompasses those contributions to the code that do not imply any major change or addition. For example, fixing a bug, adding methods to existing classes, adding new utility classes, etc. This contributions are made through a [pull request](https://help.github.com/articles/using-pull-requests/), which will be examined by the core group of developers of the project. \r\n\r\n* (B) **Major Extensions** refers to those contributions which aims to be a new functionality of the toolbox. For example,new inference methods, new learning algorithms, new concept-drift detection methods, new PGMs, new links to other toolboxes, etc. These extensions or new functionalities will be integrated as new Maven modules and will be located in the folder *[project-root-folder]/extensions/*. Then, contributing with a new extension will be based on the following three steps: (i) create a new Maven module using IntelliJ (follow this [link](https://www.jetbrains.com/idea/help/creating-maven-module.html) for details); then (ii) code your new algorithm inside this module; and (iii) make a [pull request](https://help.github.com/articles/using-pull-requests/) to add the new functionality to the project repository. \r\n        All the provided extensions should fulfill the following basic quality requirements to be accepted as extensions by the AMIDST core team. \r\n    * (1) They should contain a readme.txt file detailing the functionality and scope of the extension. It is also needs to specify if it is supported by a companion paper, student project, etc.\r\n\r\n    * (2) The code should be well documented following [JavaDoc](https://en.wikipedia.org/wiki/Javadoc) standards. \r\n \r\n    * (3) It has to include [JUnit](www.junit.org/) tests which verify the correctness of the results produced the provided code. \r\n\r\n* (C) **Use-Cases** refers to those contributions which do not add any specific functionality to the toolbox. They can be seen as examples of how this toolbox can be used. This category might include contributions related to student projects, research papers, industry applications, etc. The AMIDST core team will not supervise the quality of the contributions, this is responsibility of the contributors. They  will be integrated as independent Maven modules and will be placed in an different code repository on github, https://github.com/amidst/toolbox-use-cases/, where they are expected to be submitted using, again, a [pull request](https://help.github.com/articles/using-pull-requests/) approach. \r\n\r\n[[Back to Top]](#documentation)\r\n\r\n\r\n## Code Examples<a name=\"examples\"></a>\r\n\r\n## Data Streams<a name=\"datastreamsexample\"></a>\r\n  \r\nIn this example we show how to use the main features of a *DataStream* object. More precisely,  we show six different ways of iterating over the data samples of a *DataStream* object.\r\n\r\n\r\n```java\r\n//We can open the data stream using the static class DataStreamLoader\r\nDataStream<DataInstance> data = DataStreamLoader.openFromFile(\"datasets/SmallDataSet.arff\");\r\n\r\n//Access to the attributes defining the data set\r\nSystem.out.println(\"Attributes defining the data set\");\r\nfor (Attribute attribute : data.getAttributes()) {\r\n    System.out.println(attribute.getName());\r\n}\r\nAttribute attA = data.getAttributes().getAttributeByName(\"A\");\r\n\r\n//1. Iterating over samples using a for loop\r\nSystem.out.println(\"1. Iterating over samples using a for loop\");\r\nfor (DataInstance dataInstance : data) {\r\n    System.out.println(\"The value of attribute A for the current data instance is: \" +\r\n                                                          dataInstance.getValue(attA));\r\n}\r\n\r\n\r\n//2. Iterating using streams. We need to restart the data again \r\n//   as a DataStream can only be used once.\r\nSystem.out.println(\"2. Iterating using streams.\");\r\ndata.restart();\r\ndata.stream().forEach(dataInstance ->\r\n                System.out.println(\"The value of attribute A for the current data \"+\r\n                                        instance is: \" + dataInstance.getValue(attA))\r\n);\r\n\r\n\r\n//3. Iterating using parallel streams.\r\nSystem.out.println(\"3. Iterating using parallel streams.\");\r\ndata.restart();\r\ndata.parallelStream(10).forEach(dataInstance ->\r\n                System.out.println(\"The value of attribute A for the current data \"+\r\n                                        instance is: \" + dataInstance.getValue(attA))\r\n);\r\n\r\n//4. Iterating over a stream of data batches.\r\nSystem.out.println(\"4. Iterating over a stream of data batches.\");\r\ndata.restart();\r\ndata.streamOfBatches(10).forEach(batch -> {\r\n    for (DataInstance dataInstance : batch)\r\n                System.out.println(\"The value of attribute A for the current data \"+\r\n                                        instance is: \" + dataInstance.getValue(attA))\r\n});\r\n\r\n//5. Iterating over a parallel stream of data batches.\r\nSystem.out.println(\"5. Iterating over a parallel stream of data batches.\");\r\ndata.restart();\r\ndata.parallelStreamOfBatches(10).forEach(batch -> {\r\n    for (DataInstance dataInstance : batch)\r\n                System.out.println(\"The value of attribute A for the current data \"+\r\n                                        instance is: \" + dataInstance.getValue(attA))\r\n});\r\n\r\n\r\n//6. Iterating over data batches using a for loop\r\nSystem.out.println(\"6. Iterating over data batches using a for loop.\");\r\nfor (DataOnMemory<DataInstance> batch : data.iterableOverBatches(10)) {\r\n    for (DataInstance dataInstance : batch)\r\n                System.out.println(\"The value of attribute A for the current data \"+\r\n                                        instance is: \" + dataInstance.getValue(attA))\r\n}\r\n```\r\n\r\n[[Back to Top]](#documentation)\r\n\r\n## Random Variables<a name=\"variablesexample\"></a>\r\n\r\nThis example show the basic functionality of the classes Variables and Variable.\r\n\r\n```java\r\n//We first create an empty Variables object\r\nVariables variables = new Variables();\r\n\r\n//We invoke the \"new\" methods of the object Variables to create new variables.\r\n//Now we create a Gaussian variables\r\nVariable gaussianVar = variables.newGaussianVariable(\"Gaussian\");\r\n\r\n//Now we create a Multinomial variable with two states\r\nVariable multinomialVar = variables.newMultionomialVariable(\"Multinomial\", 2);\r\n\r\n//Now we create a Multinomial variable with two states: TRUE and FALSE\r\nVariable multinomialVar2 = variables.newMultionomialVariable(\"Multinomial2\", \r\n                                                Arrays.asList(\"TRUE, FALSE\"));\r\n\r\n//For Multinomial variables we can iterate over their different states\r\nFiniteStateSpace states = multinomialVar2.getStateSpaceType();\r\nstates.getStatesNames().forEach(System.out::println);\r\n\r\n//Variable objects can also be used, for example, to know if one variable \r\n//can be set as parent of some other variable\r\nSystem.out.println(\"Can a Gaussian variable be parent of Multinomial variable? \" +\r\n        (multinomialVar.getDistributionType().isParentCompatible(gaussianVar)));\r\n\r\nSystem.out.println(\"Can a Multinomial variable be parent of Gaussian variable? \" +\r\n        (gaussianVar.getDistributionType().isParentCompatible(multinomialVar)));\r\n```\r\n\r\n\r\n[[Back to Top]](#documentation)\r\n\r\n\r\n## Bayesian Networks<a name=\"bnexample\"></a>\r\n\r\n### Creating Bayesian Networks<a name=\"bnnohiddenexample\"></a>\r\n\r\nIn this example, we take a data set, create a BN and we compute the log-likelihood of all the samples\r\nof this data set. The numbers defining the probability distributions of the BN are randomly fixed.\r\n\r\n```java\r\n//We can open the data stream using the static class DataStreamLoader\r\nDataStream<DataInstance> data = DataStreamLoader.openFromFile(\"datasets/syntheticData.arff\");\r\n\r\n\r\n/**\r\n * 1. Once the data is loaded, we create a random variable for each of the attributes (i.e. data columns)\r\n * in our data.\r\n *\r\n * 2. {@link Variables} is the class for doing that. It takes a list of Attributes and internally creates\r\n * all the variables. We create the variables using Variables class to guarantee that each variable\r\n * has a different ID number and make it transparent for the user.\r\n *\r\n * 3. We can extract the Variable objects by using the method getVariableByName();\r\n */\r\nVariables variables = new Variables(data.getAttributes());\r\n\r\nVariable a = variables.getVariableByName(\"A\");\r\nVariable b = variables.getVariableByName(\"B\");\r\nVariable c = variables.getVariableByName(\"C\");\r\nVariable d = variables.getVariableByName(\"D\");\r\nVariable e = variables.getVariableByName(\"E\");\r\nVariable g = variables.getVariableByName(\"G\");\r\nVariable h = variables.getVariableByName(\"H\");\r\nVariable i = variables.getVariableByName(\"I\");\r\n\r\n/**\r\n * 1. Once you have defined your {@link Variables} object, the next step is to create\r\n * a DAG structure over this set of variables.\r\n *\r\n * 2. To add parents to each variable, we first recover the ParentSet object by the method\r\n * getParentSet(Variable var) and then call the method addParent().\r\n */\r\nDAG dag = new DAG(variables);\r\n\r\ndag.getParentSet(e).addParent(a);\r\ndag.getParentSet(e).addParent(b);\r\n\r\ndag.getParentSet(h).addParent(a);\r\ndag.getParentSet(h).addParent(b);\r\n\r\ndag.getParentSet(i).addParent(a);\r\ndag.getParentSet(i).addParent(b);\r\ndag.getParentSet(i).addParent(c);\r\ndag.getParentSet(i).addParent(d);\r\n\r\ndag.getParentSet(g).addParent(c);\r\ndag.getParentSet(g).addParent(d);\r\n\r\n/**\r\n * 1. We first check if the graph contains cycles.\r\n *\r\n * 2. We print out the created DAG. We can check that everything is as expected.\r\n */\r\nif (dag.containCycles()) {\r\n    try {\r\n    } catch (Exception ex) {\r\n        throw new IllegalArgumentException(ex);\r\n    }\r\n}\r\n\r\nSystem.out.println(dag.toString());\r\n\r\n\r\n/**\r\n * 1. We now create the Bayesian network from the previous DAG.\r\n *\r\n * 2. The BN object is created from the DAG. It automatically looks at the distribution tye\r\n * of each variable and their parents to initialize the Distributions objects that are stored\r\n * inside (i.e. Multinomial, Normal, CLG, etc). The parameters defining these distributions are\r\n * properly initialized.\r\n *\r\n * 3. The network is printed and we can have look at the kind of distributions stored in the BN object.\r\n */\r\nBayesianNetwork bn = new BayesianNetwork(dag);\r\nSystem.out.println(bn.toString());\r\n\r\n\r\n/**\r\n * 1. We iterate over the data set sample by sample.\r\n *\r\n * 2. For each sample or DataInstance object, we compute the log of the probability that the BN object\r\n * assigns to this observation.\r\n *\r\n * 3. We accumulate these log-probs and finally we print the log-prob of the data set.\r\n */\r\ndouble logProb = 0;\r\nfor (DataInstance instance : data) {\r\n    logProb += bn.getLogProbabiltyOf(instance);\r\n}\r\nSystem.out.println(logProb);\r\n\r\nBayesianNetworkWriter.saveToFile(bn, \"networks/BNExample.bn\");\r\n```\r\n\r\n[[Back to Top]](#documentation)\r\n\r\n\r\n### Creating Bayesian Networks with latent variables <a name=\"bnhiddenexample\"></a>\r\n\r\nIn this example, we simply show how to create a BN model with hidden variables. We simply create a BN for clustering, i.e.,  a naive-Bayes like structure with a single common hidden variable acting as parant of all the observable variables.\r\n \r\n```java\r\n//We can open the data stream using the static class DataStreamLoader\r\nDataStream<DataInstance> data = DataStreamLoader.openFromFile(\"datasets/syntheticData.arff\");\r\n\r\n/**\r\n * 1. Once the data is loaded, we create a random variable for each of the attributes (i.e. data columns)\r\n * in our data.\r\n *\r\n * 2. {@link Variables} is the class for doing that. It takes a list of Attributes and internally creates\r\n * all the variables. We create the variables using Variables class to guarantee that each variable\r\n * has a different ID number and make it transparent for the user.\r\n *\r\n * 3. We can extract the Variable objects by using the method getVariableByName();\r\n */\r\nVariables variables = new Variables(data.getAttributes());\r\n\r\nVariable a = variables.getVariableByName(\"A\");\r\nVariable b = variables.getVariableByName(\"B\");\r\nVariable c = variables.getVariableByName(\"C\");\r\nVariable d = variables.getVariableByName(\"D\");\r\nVariable e = variables.getVariableByName(\"E\");\r\nVariable g = variables.getVariableByName(\"G\");\r\nVariable h = variables.getVariableByName(\"H\");\r\nVariable i = variables.getVariableByName(\"I\");\r\n\r\n/**\r\n * 1. We create the hidden variable. For doing that we make use of the method \"newMultionomialVariable\". When\r\n * a variable is created from an Attribute object, it contains all the information we need (e.g.\r\n * the name, the type, etc). But hidden variables does not have an associated attribute\r\n * and, for this reason, we use now this to provide this information.\r\n *\r\n * 2. Using the \"newMultionomialVariable\" method, we define a variable called HiddenVar, which is\r\n * not associated to any attribute and, then, it is a latent variable, its state space is a finite set with two elements, and its\r\n * distribution type is multinomial.\r\n *\r\n * 3. We finally create the hidden variable using the method \"newVariable\".\r\n */\r\n\r\nVariable hidden = variables.newMultionomialVariable(\"HiddenVar\", Arrays.asList(\"TRUE\", \"FALSE\"));\r\n\r\n/**\r\n * 1. Once we have defined your {@link Variables} object, including the latent variable,\r\n * the next step is to create a DAG structure over this set of variables.\r\n *\r\n * 2. To add parents to each variable, we first recover the ParentSet object by the method\r\n * getParentSet(Variable var) and then call the method addParent(Variable var).\r\n *\r\n * 3. We just put the hidden variable as parent of all the other variables. Following a naive-Bayes\r\n * like structure.\r\n */\r\nDAG dag = new DAG(variables);\r\n\r\ndag.getParentSet(a).addParent(hidden);\r\ndag.getParentSet(b).addParent(hidden);\r\ndag.getParentSet(c).addParent(hidden);\r\ndag.getParentSet(d).addParent(hidden);\r\ndag.getParentSet(e).addParent(hidden);\r\ndag.getParentSet(g).addParent(hidden);\r\ndag.getParentSet(h).addParent(hidden);\r\ndag.getParentSet(i).addParent(hidden);\r\n\r\n/**\r\n * We print the graph to see if is properly created.\r\n */\r\nSystem.out.println(dag.toString());\r\n\r\n/**\r\n * 1. We now create the Bayesian network from the previous DAG.\r\n *\r\n * 2. The BN object is created from the DAG. It automatically looks at the distribution type\r\n * of each variable and their parents to initialize the Distributions objects that are stored\r\n * inside (i.e. Multinomial, Normal, CLG, etc). The parameters defining these distributions are\r\n * properly initialized.\r\n *\r\n * 3. The network is printed and we can have look at the kind of distributions stored in the BN object.\r\n */\r\nBayesianNetwork bn = new BayesianNetwork(dag);\r\nSystem.out.println(bn.toString());\r\n\r\n/**\r\n * Finally teh Bayesian network is saved to a file.\r\n */\r\nBayesianNetworkWriter.saveToFile(bn, \"networks/BNHiddenExample.bn\");\r\n```\r\n\r\n[[Back to Top]](#documentation)\r\n\r\n\r\n### Modifying Bayesian Networks <a name=\"bnmodifyexample\"></a>\r\n\r\nIn this example we show how to access and modify the conditional probabilities of a Bayesian network model.\r\n\r\n```java\r\n//We first generate a Bayesian network with one multinomial, one Gaussian variable and one link.\r\nBayesianNetworkGenerator.setNumberOfGaussianVars(1);\r\nBayesianNetworkGenerator.setNumberOfMultinomialVars(1,2);\r\nBayesianNetworkGenerator.setNumberOfLinks(1);\r\n\r\nBayesianNetwork bn = BayesianNetworkGenerator.generateBayesianNetwork();\r\n\r\n//We print the randomly generated Bayesian networks\r\nSystem.out.println(bn.toString());\r\n\r\n//We first access the variable we are interested in\r\nVariable multiVar = bn.getStaticVariables().getVariableByName(\"DiscreteVar0\");\r\n\r\n//Using the above variable we can get the associated distribution and modify it\r\nMultinomial multinomial = bn.getConditionalDistribution(multiVar);\r\nmultinomial.setProbabilities(new double[]{0.2, 0.8});\r\n\r\n//Same than before but accessing the another variable\r\nVariable normalVar = bn.getStaticVariables().getVariableByName(\"GaussianVar0\");\r\n\r\n//In this case, the conditional distribtuion is of the type \"Normal given Multinomial Parents\"\r\nNormal_MultinomialParents normalMultiDist = bn.getConditionalDistribution(normalVar);\r\nnormalMultiDist.getNormal(0).setMean(1.0);\r\nnormalMultiDist.getNormal(0).setVariance(1.0);\r\n\r\nnormalMultiDist.getNormal(1).setMean(0.0);\r\nnormalMultiDist.getNormal(1).setVariance(1.0);\r\n\r\n//We print modified Bayesian network\r\nSystem.out.println(bn.toString());\r\n```\r\n\r\n[[Back to Top]](#documentation)\r\n\r\n## I/O Functionality <a name=\"ioexample\"></a>\r\n\r\n### I/O of Data Streams <a name=\"iodatastreamsexample\"></a>\r\n\r\nIn this example we show how to load and save data sets from [.arff](http://www.cs.waikato.ac.nz/ml/weka/arff.html) files. \r\n\r\n```java\r\n//We can open the data stream using the static class DataStreamLoader\r\nDataStream<DataInstance> data = DataStreamLoader.openFromFile(\"datasets/syntheticData.arff\");\r\n\r\n//We can save this data set to a new file using the static class DataStreamWriter\r\nDataStreamWriter.writeDataToFile(data, \"datasets/tmp.arff\");\r\n```\r\n\r\n[[Back to Top]](#documentation)\r\n\r\n### I/O of Bayesian Networks <a name=\"iobnsexample\"></a>\r\n\r\n\r\nIn this example we show how to load and save Bayesian networks models for a binary file with \".bn\" extension. In this toolbox Bayesian networks models are saved as serialized objects.\r\n\r\n```java\r\n//We can load a Bayesian network using the static class BayesianNetworkLoader\r\nBayesianNetwork bn = BayesianNetworkLoader.loadFromFile(\"./networks/WasteIncinerator.bn\");\r\n\r\n//Now we print the loaded model\r\nSystem.out.println(bn.toString());\r\n\r\n//Now we change the parameters of the model\r\nbn.randomInitialization(new Random(0));\r\n\r\n//We can save this Bayesian network to using the static class BayesianNetworkWriter\r\nBayesianNetworkWriter.saveToFile(bn, \"networks/tmp.bn\");\r\n```\r\n\r\n[[Back to Top]](#documentation)\r\n\r\n## Inference Algorithms <a name=\"inferenceexample\"></a>\r\n\r\n### The Inference Engine <a name=\"inferenceengingeexample\"></a>\r\n\r\nThis example show how to perform inference in a Bayesian network model using the InferenceEngine static class. This class aims to be a straigthfoward way to perform queries over a Bayesian network model. By the default the \\textit{VMP} inference method is invoked.\r\n\r\n```java\r\n//We first load the WasteIncinerator bayesian network which has multinomial \r\n//and Gaussian variables.\r\nBayesianNetwork bn = BayesianNetworkLoader.loadFromFile(\"./networks/WasteIncinerator.bn\");\r\n\r\n//We recover the relevant variables for this example: Mout which is normally \r\n//distributed, and W which is multinomial.\r\nVariable varMout = bn.getStaticVariables().getVariableByName(\"Mout\");\r\nVariable varW = bn.getStaticVariables().getVariableByName(\"W\");\r\n\r\n//Set the evidence.\r\nAssignment assignment = new HashMapAssignment(1);\r\nassignment.setValue(varW,0);\r\n\r\n//Then we query the posterior of\r\nSystem.out.println(\"P(Mout|W=0) = \" + InferenceEngine.getPosterior(varMout, bn, assignment));\r\n\r\n//Or some more refined queries\r\nSystem.out.println(\"P(0.7<Mout<6.59 | W=0) = \" + \r\n InferenceEngine.getExpectedValue(varMout, bn, v -> (0.7 < v && v < 6.59) ? 1.0 : 0.0 ));\r\n```\r\n\r\n[[Back to Top]](#documentation)\r\n\r\n### Variational Message Passing <a name=\"vmpexample\"></a>\r\n\r\nThis example we show how to perform inference on a general Bayesian network using the Variational Message Passing (VMP)\r\nalgorithm detailed in\r\n\r\n> Winn, J. M., Bishop, C. M. (2005). Variational message passing. In Journal of Machine Learning Research (pp. 661-694).\r\n\r\n\r\n\r\n```java\r\n//We first load the WasteIncinerator bayesian network which has multinomial \r\n//and Gaussian variables.\r\nBayesianNetwork bn = BayesianNetworkLoader.loadFromFile(\"./networks/WasteIncinerator.bn\");\r\n\r\n//We recover the relevant variables for this example: Mout which is normally \r\n//distributed, and W which is multinomial.\r\nVariable varMout = bn.getStaticVariables().getVariableByName(\"Mout\");\r\nVariable varW = bn.getStaticVariables().getVariableByName(\"W\");\r\n\r\n//First we create an instance of a inference algorithm. In this case, we use \r\n//the VMP class.\r\nInferenceAlgorithm inferenceAlgorithm = new VMP();\r\n\r\n//Then, we set the BN model\r\ninferenceAlgorithm.setModel(bn);\r\n\r\n//If exists, we also set the evidence.\r\nAssignment assignment = new HashMapAssignment(1);\r\nassignment.setValue(varW,0);\r\ninferenceAlgorithm.setEvidence(assignment);\r\n\r\n//Then we run inference\r\ninferenceAlgorithm.runInference();\r\n\r\n//Then we query the posterior of\r\nSystem.out.println(\"P(Mout|W=0) = \" + inferenceAlgorithm.getPosterior(varMout));\r\n\r\n//Or some more refined queries\r\nSystem.out.println(\"P(0.7<Mout<6.59 | W=0) = \" + \r\n inferenceAlgorithm.getExpectedValue(varMout, v -> (0.7 < v && v < 6.59) ? 1.0 : 0.0 ));\r\n\r\n//We can also compute the probability of the evidence\r\nSystem.out.println(\"P(W=0) = \" + Math.exp(inferenceAlgorithm.getLogProbabilityOfEvidence()));\r\n```\r\n\r\n[[Back to Top]](#documentation)\r\n\r\n### Importance Sampling <a name=\"isexample\"></a>\r\n\r\nThis example we show how to perform inference on a general Bayesian network using an importance sampling\r\nalgorithm detailed in\r\n\r\n>Fung, R., Chang, K. C. (2013). Weighing and integrating evidence for stochastic simulation in Bayesian networks. arXiv preprint arXiv:1304.1504.\r\n\r\n```java\r\n//We first load the WasteIncinerator bayesian network which has multinomial \r\n//and Gaussian variables.\r\nBayesianNetwork bn = BayesianNetworkLoader.loadFromFile(\"./networks/WasteIncinerator.bn\");\r\n\r\n//We recover the relevant variables for this example: Mout which is normally \r\n//distributed, and W which is multinomial.\r\nVariable varMout = bn.getStaticVariables().getVariableByName(\"Mout\");\r\nVariable varW = bn.getStaticVariables().getVariableByName(\"W\");\r\n\r\n//First we create an instance of a inference algorithm. In this case, we use \r\n//the ImportanceSampling class.\r\nInferenceAlgorithm inferenceAlgorithm = new ImportanceSampling();\r\n\r\n//Then, we set the BN model\r\ninferenceAlgorithm.setModel(bn);\r\n\r\n//If exists, we also set the evidence.\r\nAssignment assignment = new HashMapAssignment(1);\r\nassignment.setValue(varW,0);\r\ninferenceAlgorithm.setEvidence(assignment);\r\n\r\n//We can also set to be run in parallel on multicore CPUs\r\ninferenceAlgorithm.setParallelMode(true);\r\n\r\n//Then we run inference\r\ninferenceAlgorithm.runInference();\r\n\r\n//Then we query the posterior of\r\nSystem.out.println(\"P(Mout|W=0) = \" + inferenceAlgorithm.getPosterior(varMout));\r\n\r\n//Or some more refined queries\r\nSystem.out.println(\"P(0.7<Mout<6.59 | W=0) = \" + \r\n inferenceAlgorithm.getExpectedValue(varMout, v -> (0.7 < v && v < 6.59) ? 1.0 : 0.0 ));\r\n\r\n//We can also compute the probability of the evidence\r\nSystem.out.println(\"P(W=0) = \" + Math.exp(inferenceAlgorithm.getLogProbabilityOfEvidence()));\r\n```\r\n\r\n[[Back to Top]](#documentation)\r\n\r\n## Learning Algorithms <a name=\"learningexample\"></a>\r\n### Maximum Likelihood <a name=\"mlexample\"></a>\r\n\r\n\r\nThis other example shows how to learn incrementally the parameters of a Bayesian network using data batches,\r\n\r\n```java\r\n//We can open the data stream using the static class DataStreamLoader\r\nDataStream<DataInstance> data = \r\n                  DataStreamLoader.openFromFile(\"datasets/WasteIncineratorSample.arff\");\r\n\r\n//We create a ParameterLearningAlgorithm object with the MaximumLikehood builder\r\nParameterLearningAlgorithm parameterLearningAlgorithm = new ParallelMaximumLikelihood();\r\n\r\n//We fix the DAG structure\r\nparameterLearningAlgorithm.setDAG(getNaiveBayesStructure(data,0));\r\n\r\n//We should invoke this method before processing any data\r\nparameterLearningAlgorithm.initLearning();\r\n\r\n\r\n//Then we show how we can perform parameter learnig by a sequential updating of data batches.\r\nfor (DataOnMemory<DataInstance> batch : data.iterableOverBatches(100)){\r\n    parameterLearningAlgorithm.updateModel(batch);\r\n}\r\n\r\n//And we get the model\r\nBayesianNetwork bnModel = parameterLearningAlgorithm.getLearntBayesianNetwork();\r\n\r\n//We print the model\r\nSystem.out.println(bnModel.toString());\r\n```\r\n\r\n[[Back to Top]](#documentation)\r\n\r\n### Parallel Maximum Likelihood <a name=\"pmlexample\"></a>\r\n\r\nThis example shows how to learn in parallel the parameters of a Bayesian network from a stream of data using maximum likelihood.\r\n\r\n```java\r\n//We can open the data stream using the static class DataStreamLoader\r\nDataStream<DataInstance> data = \r\n           DataStreamLoader.openFromFile(\"datasets/syntheticData.arff\");\r\n\r\n//We create a MaximumLikelihood object with the MaximumLikehood builder\r\nMaximumLikelihood parameterLearningAlgorithm = new MaximumLikelihood();\r\n\r\n//We activate the parallel mode.\r\nparameterLearningAlgorithm.setParallelMode(true);\r\n\r\n//We fix the DAG structure\r\nparameterLearningAlgorithm.setDAG(getNaiveBayesStructure(data,0));\r\n\r\n//We set the batch size which will be employed to learn the model in parallel\r\nparameterLearningAlgorithm.setBatchSize(100);\r\n\r\n//We set the data which is going to be used for leaning the parameters\r\nparameterLearningAlgorithm.setDataStream(data);\r\n\r\n//We perform the learning\r\nparameterLearningAlgorithm.runLearning();\r\n\r\n//And we get the model\r\nBayesianNetwork bnModel = parameterLearningAlgorithm.getLearntBayesianNetwork();\r\n\r\n//We print the model\r\nSystem.out.println(bnModel.toString());\r\n```\r\n\r\n\r\n[[Back to Top]](#documentation)\r\n\r\n### Streaming Variational Bayes <a name=\"svbexample\"></a>\r\n\r\nThis example shows how to learn incrementally the parameters of a Bayesian network from a stream of data with a Bayesian approach using the following algorithm,\r\n\r\n>Broderick, T., Boyd, N., Wibisono, A., Wilson, A. C., \\& Jordan, M. I. (2013). Streaming variational Bayes. \r\nIn Advances in Neural Information Processing Systems (pp. 1727-1735).\r\n\r\nIn this second example we show a alternative implementation which explicitly updates the model by batches by using the class *SVB*.\r\n\r\n\r\n```java\r\n//We can open the data stream using the static class DataStreamLoader\r\nDataStream<DataInstance> data = \r\n                      DataStreamLoader.openFromFile(\"datasets/WasteIncineratorSample.arff\");\r\n\r\n//We create a StreamingVariationalBayesVMP object\r\nStreamingVariationalBayesVMP parameterLearningAlgorithm = new StreamingVariationalBayesVMP();\r\n\r\n//We fix the DAG structure, which is a Naive Bayes with a \r\n//global latent binary variable\r\nparameterLearningAlgorithm.setDAG(getHiddenNaiveBayesStructure(data));\r\n\r\n//We fix the size of the window, which must be equal to the size of the data batches \r\n//we use for learning\r\nparameterLearningAlgorithm.setWindowsSize(5);\r\n\r\n//We can activate the output\r\nparameterLearningAlgorithm.setOutput(true);\r\n\r\n//We should invoke this method before processing any data\r\nparameterLearningAlgorithm.initLearning();\r\n\r\n//Then we show how we can perform parameter learnig by a sequential updating of \r\n//data batches.\r\nfor (DataOnMemory<DataInstance> batch : data.iterableOverBatches(5)){\r\n    parameterLearningAlgorithm.updateModel(batch);\r\n}\r\n\r\n//And we get the model\r\nBayesianNetwork bnModel = parameterLearningAlgorithm.getLearntBayesianNetwork();\r\n\r\n//We print the model\r\nSystem.out.println(bnModel.toString());\r\n```\r\n\r\n[[Back to Top]](#documentation)\r\n\r\n### Parallel Streaming Variational Bayes <a name=\"psvbexample\"></a>\r\n\r\nThis example shows how to learn in the parameters of a Bayesian network from a stream of data with a Bayesian\r\napproach using the parallel version  of the SVB algorithm, \r\n\r\n>Broderick, T., Boyd, N., Wibisono, A., Wilson, A. C., \\& Jordan, M. I. (2013). Streaming variational Bayes. \r\nIn Advances in Neural Information Processing Systems (pp. 1727-1735).\r\n\r\n```java\r\n//We can open the data stream using the static class DataStreamLoader\r\nDataStream<DataInstance> data = \r\n                   DataStreamLoader.openFromFile(\"datasets/WasteIncineratorSample.arff\");\r\n\r\n//We create a ParallelSVB object\r\nParallelSVB parameterLearningAlgorithm = new ParallelSVB();\r\n\r\n//We fix the number of cores we want to exploit\r\nparameterLearningAlgorithm.setNCores(4);\r\n\r\n//We fix the DAG structure, which is a Naive Bayes with a \r\n//global latent binary variable\r\nparameterLearningAlgorithm.setDAG(StreamingVMPExample.getHiddenNaiveBayesStructure(data));\r\n\r\n\r\n//We fix the size of the window\r\nparameterLearningAlgorithm.getSVBEngine().setWindowsSize(100);\r\n\r\n//We can activate the output\r\nparameterLearningAlgorithm.setOutput(true);\r\n\r\n//We set the data which is going to be used for leaning the parameters\r\nparameterLearningAlgorithm.setDataStream(data);\r\n\r\n//We perform the learning\r\nparameterLearningAlgorithm.runLearning();\r\n\r\n//And we get the model\r\nBayesianNetwork bnModel = parameterLearningAlgorithm.getLearntBayesianNetwork();\r\n\r\n//We print the model\r\nSystem.out.println(bnModel.toString());\r\n```\r\n\r\n[[Back to Top]](#documentation)\r\n\r\n## Concept Drift Methods <a name=\"conceptdriftexample\"></a>\r\n\r\n### Maximum Likelihood with Fading <a name=\"mlfadingexample\"></a>\r\n\r\nThis example shows how to adaptively learn the parameters of a Bayesian network from a stream of data using exponential forgetting with a given fading factor, directly inspired by the approach presented in\r\n\r\n>Olesen, K. G., Lauritzen, S. L., \\& Jensen, F. V. (1992, July). aHUGIN: A system creating adaptive causal probabilistic networks. In Proceedings of the Eighth international conference on Uncertainty in Artificial Intelligence (pp. 223-229). Morgan Kaufmann Publishers Inc.\r\n\r\n```java\r\n//We can open the data stream using the static class DataStreamLoader\r\nDataStream<DataInstance> data = \r\n             DataStreamLoader.openFromFile(\"datasets/WasteIncineratorSample.arff\");\r\n\r\n//We create a ParameterLearningAlgorithm object with \r\n//the MaximumLikelihoodFading builder\r\nMaximumLikelihoodFading parameterLearningAlgorithm = new MaximumLikelihoodFading();\r\n\r\n//We fix the DAG structure\r\nparameterLearningAlgorithm.setDAG(\r\n                MaximimumLikelihoodByBatchExample.getNaiveBayesStructure(data, 0));\r\n\r\n//We fix the fading or forgeting factor\r\nparameterLearningAlgorithm.setFadingFactor(0.9);\r\n\r\n//We set the batch size which will be employed to learn the model\r\nparameterLearningAlgorithm.setBatchSize(100);\r\n\r\n//We set the data which is going to be used for leaning the parameters\r\nparameterLearningAlgorithm.setDataStream(data);\r\n\r\n//We perform the learning\r\nparameterLearningAlgorithm.runLearning();\r\n\r\n//And we get the model\r\nBayesianNetwork bnModel = parameterLearningAlgorithm.getLearntBayesianNetwork();\r\n\r\n//We print the model\r\nSystem.out.println(bnModel.toString());\r\n```\r\n\r\n[[Back to Top]](#documentation)\r\n\r\n### Streaming Variational Bayes with Fading <a name=\"svbfadingexample\"></a>\r\n\r\nThis example shows how to adaptively learn in the parameters of a Bayesian network from a stream of data with a Bayesian approach using a combination of the the following two methods,\r\n\r\n>Broderick, T., Boyd, N., Wibisono, A., Wilson, A. C., \\& Jordan, M. I. (2013). Streaming variational Bayes. \r\nIn Advances in Neural Information Processing Systems (pp. 1727-1735).\r\n\r\n>Olesen, K. G., Lauritzen, S. L., \\& Jensen, F. V. (1992, July). aHUGIN: A system creating adaptive causal probabilistic networks. In Proceedings of the Eighth international conference on Uncertainty in Artificial Intelligence (pp. 223-229). Morgan Kaufmann Publishers Inc.\r\n\r\n```java\r\n//We can open the data stream using the static class DataStreamLoader\r\nDataStream<DataInstance> data = \r\n           DataStreamLoader.openFromFile(\"datasets/WasteIncineratorSample.arff\");\r\n\r\n//We create a SVB object\r\nSVBFading parameterLearningAlgorithm = new SVBFading();\r\n\r\n//We fix the DAG structure\r\nparameterLearningAlgorithm.setDAG(SVBExample.getHiddenNaiveBayesStructure(data));\r\n\r\n//We fix the fading or forgeting factor\r\nparameterLearningAlgorithm.setFadingFactor(0.9);\r\n\r\n//We fix the size of the window\r\nparameterLearningAlgorithm.setWindowsSize(100);\r\n\r\n//We can activate the output\r\nparameterLearningAlgorithm.setOutput(true);\r\n\r\n//We set the data which is going to be used for leaning the parameters\r\nparameterLearningAlgorithm.setDataStream(data);\r\n\r\n//We perform the learning\r\nparameterLearningAlgorithm.runLearning();\r\n\r\n//And we get the model\r\nBayesianNetwork bnModel = parameterLearningAlgorithm.getLearntBayesianNetwork();\r\n\r\n//We print the model\r\nSystem.out.println(bnModel.toString());\r\n```\r\n\r\n[[Back to Top]](#documentation)\r\n\r\n### Naive Bayes with Virtual Concept Drift Detection <a name=\"nbconceptdriftexample\"></a>\r\n\r\nThis example shows how to use the class NaiveBayesVirtualConceptDriftDetector to run the virtual concept drift detector detailed in\r\n\r\n> Borchani et al. Modeling concept drift: A probabilistic graphical model based approach. IDA 2015.\r\n\r\n```java\r\n//We can open the data stream using the static class DataStreamLoader\r\nDataStream<DataInstance> data = DataStreamLoader.openFromFile(\"./datasets/DriftSets/sea.arff\");\r\n\r\n//We create a NaiveBayesVirtualConceptDriftDetector object\r\nNaiveBayesVirtualConceptDriftDetector virtualDriftDetector = \r\n                                        new NaiveBayesVirtualConceptDriftDetector();\r\n\r\n//We set class variable as the last attribute\r\nvirtualDriftDetector.setClassIndex(-1);\r\n\r\n//We set the data which is going to be used\r\nvirtualDriftDetector.setData(data);\r\n\r\n//We fix the size of the window\r\nint windowSize = 1000;\r\nvirtualDriftDetector.setWindowsSize(windowSize);\r\n\r\n//We fix the so-called transition variance\r\nvirtualDriftDetector.setTransitionVariance(0.1);\r\n\r\n//We fix the number of global latent variables\r\nvirtualDriftDetector.setNumberOfGlobalVars(1);\r\n\r\n//We should invoke this method before processing any data\r\nvirtualDriftDetector.initLearning();\r\n\r\n//Some prints\r\nSystem.out.print(\"Batch\");\r\nfor (Variable hiddenVar : virtualDriftDetector.getHiddenVars()) {\r\n    System.out.print(\"\\t\" + hiddenVar.getName());\r\n}\r\nSystem.out.println();\r\n\r\n\r\n//Then we show how we can perform the sequential processing of\r\n// data batches. They must be of the same value than the window\r\n// size parameter set above.\r\nint countBatch = 0;\r\nfor (DataOnMemory<DataInstance> batch : \r\n\t\t\t\tdata.iterableOverBatches(windowSize)){\r\n\r\n    //We update the model by invoking this method. The output\r\n    // is an array with a value associated\r\n    // to each fo the global hidden variables\r\n    double[] out = virtualDriftDetector.updateModel(batch);\r\n\r\n    //We print the output\r\n    System.out.print(countBatch + \"\\t\");\r\n    for (int i = 0; i < out.length; i++) {\r\n        System.out.print(out[i]+\"\\t\");\r\n    }\r\n    System.out.println();\r\n    countBatch++;\r\n}\r\n```\r\n\r\n\r\n[[Back to Top]](#documentation)\r\n\r\n## HuginLink <a name=\"huginglinkexample\"></a>\r\n### Models conversion between AMIDST and Hugin <a name=\"huginglinkconversionexample\"></a>\r\n\r\nThis example shows how to use the class BNConverterToAMIDST and BNConverterToHugin to convert a \r\nBayesian network models between Hugin and AMIDST formats\r\n\r\n\r\n```java\r\n//We load from Hugin format\r\nDomain huginBN = BNLoaderFromHugin.loadFromFile(\"networks/asia.net\");\r\n\r\n//Then, it is converted to AMIDST BayesianNetwork object\r\nBayesianNetwork amidstBN = BNConverterToAMIDST.convertToAmidst(huginBN);\r\n\r\n//Then, it is converted to Hugin Bayesian Network object\r\nhuginBN = BNConverterToHugin.convertToHugin(amidstBN);\r\n\r\nSystem.out.println(amidstBN.toString());\r\nSystem.out.println(huginBN.toString());\r\n```\r\n\r\n\r\n[[Back to Top]](#documentation)\r\n\r\n### I/O of Bayesian Networks with Hugin net format <a name=\"huginglinkioexample\"></a>\r\n\r\nThis example shows how to use the class BNLoaderFromHugin and BNWriterToHugin classes to load and\r\nwrite Bayesian networks in Hugin format.\r\n\r\n```java\r\n//We load from Hugin format\r\nDomain huginBN = BNLoaderFromHugin.loadFromFile(\"networks/asia.net\");\r\n\r\n//We save a AMIDST BN to Hugin format\r\nBayesianNetwork amidstBN = BNConverterToAMIDST.convertToAmidst(huginBN);\r\nBNWriterToHugin.saveToHuginFile(amidstBN,\"networks/tmp.net\");\r\n```\r\n\r\n[[Back to Top]](#documentation)\r\n\r\n### Invoking Hugin's inference engine <a name=\"huginglinkinferenceexample\"></a>\r\n\r\nThis example we show how to perform inference using [Hugin](http://www.hugin.com) inference engine within the AMIDST toolbox\r\n\r\n```java\r\n//We first load the WasteIncinerator bayesian network \r\n//which has multinomial and Gaussian variables.\r\nBayesianNetwork bn = BayesianNetworkLoader.loadFromFile(\"./networks/WasteIncinerator.bn\");\r\n\r\n//We recover the relevant variables for this example: \r\n//Mout which is normally distributed, and W which is multinomial.\r\nVariable varMout = bn.getStaticVariables().getVariableByName(\"Mout\");\r\nVariable varW = bn.getStaticVariables().getVariableByName(\"W\");\r\n\r\n//First we create an instance of a inference algorithm. \r\n//In this case, we use the ImportanceSampling class.\r\nInferenceAlgorithm inferenceAlgorithm = new HuginInference();\r\n\r\n//Then, we set the BN model\r\ninferenceAlgorithm.setModel(bn);\r\n\r\n//If exists, we also set the evidence.\r\nAssignment assignment = new HashMapAssignment(1);\r\nassignment.setValue(varW,0);\r\ninferenceAlgorithm.setEvidence(assignment);\r\n\r\n//Then we run inference\r\ninferenceAlgorithm.runInference();\r\n\r\n//Then we query the posterior of\r\nSystem.out.println(\"P(Mout|W=0) = \" + inferenceAlgorithm.getPosterior(varMout));\r\n\r\n//Or some more refined queries\r\nSystem.out.println(\"P(0.7<Mout<3.5 | W=0) = \" \r\n   + inferenceAlgorithm.getExpectedValue(varMout, v -> (0.7 < v && v < 3.5) ? 1.0 : 0.0 ));\r\n```\r\n\r\n[[Back to Top]](#documentation)\r\n\r\n### Invoking Hugin's Parallel TAN <a name=\"huginglinkTANexample\"></a>\r\n\r\nThis example we show how to perform inference using [Hugin](http://www.hugin.com) inference engine within the AMIDST toolbox\r\n\r\n\r\nThis example shows how to use [Hugin](http://www.hugin.com)'s functionality to learn in parallel a TAN model. An important remark is that [Hugin](http://www.hugin.com) only allows to learn the TAN model for a data set completely loaded into RAM memory. The case where our data set does not fit into memory, it solved in AMIDST in the following way. We learn the structure using a smaller data set produced by [Reservoir sampling](https://en.wikipedia.org/wiki/Reservoir_sampling) and, then, we use AMIDST's [ParallelMaximumLikelihood](http://amidst.github.io/toolbox/#pmlexample) to learn the parameters of the TAN model over the whole data set.\r\n\r\nFor further details about the implementation of the parallel TAN algorithm look at the following paper:\r\n\r\n>Madsen, A.L. et al. A New Method for Vertical Parallelisation of TAN Learning Based on Balanced Incomplete Block Designs. Probabilistic Graphical Models. Lecture Notes in Computer Science Volume 8754, 2014, pp 302-317.\r\n\r\n```java\r\n//We load a Bayesian network to generate a data stream\r\n//using BayesianNewtorkSampler class.\r\nint sampleSize = 100000;\r\nBayesianNetwork bn = BayesianNetworkLoader.loadFromFile(\"networks/Pigs.bn\");\r\nBayesianNetworkSampler sampler = new BayesianNetworkSampler(bn);\r\n\r\n//We fix the number of samples in memory used for performing the structural learning.\r\n//They are randomly sub-sampled using Reservoir sampling.\r\nint samplesOnMemory = 5000;\r\n\r\n//We make different trials with different number of cores\r\nArrayList<Integer> vNumCores = new ArrayList(Arrays.asList(1, 2, 3, 4));\r\n\r\nfor (Integer numCores : vNumCores) {\r\n    System.out.println(\"Learning TAN: \" + samplesOnMemory + \" samples on memory, \" + numCores + \" core/s ...\");\r\n    DataStream<DataInstance> data = sampler.sampleToDataStream(sampleSize);\r\n\r\n    //The class ParallelTAN is created\r\n    ParallelTAN tan = new ParallelTAN();\r\n\r\n    //We activate the parallel mode.\r\n    tan.setParallelMode(true);\r\n\r\n    //We set the number of cores to be used for the structural learning\r\n    tan.setNumCores(numCores);\r\n\r\n    //We set the number of samples to be used for the learning the structure\r\n    tan.setNumSamplesOnMemory(samplesOnMemory);\r\n\r\n    //We set the root variable to be first variable\r\n    tan.setNameRoot(bn.getVariables().getListOfVariables().get(0).getName());\r\n\r\n    //We set the class variable to be the last variable\r\n    tan.setNameTarget(bn.getVariables().getListOfVariables().get(bn.getVariables().getListOfVariables().size()-1).getName());\r\n\r\n    Stopwatch watch = Stopwatch.createStarted();\r\n\r\n    //We just invoke this mode to learn the TAN model for the data stream\r\n    BayesianNetwork model = tan.learn(data);\r\n\r\n    System.out.println(watch.stop());\r\n}\r\n```\r\n\r\n\r\n## MoaLink <a name=\"moalinkexample\"></a>\r\n### AMIDST Classifiers from MOA <a name=\"moalinkclassifiersexample\"></a>\r\n\r\nThe following command can be used to learn a Bayesian model with a latent Gaussian variable (HG) and a multinomial with 2 states (HM), as displayed in figure below. The VMP algorithm is used to learn the parameters of these two non-observed variables and make predictions over the class variable.\r\n\r\n\r\n<p align=\"center\">\r\n<img title=\"Taxonomy\" src=\"http://amidst.github.io/toolbox/images/HODE.jpg\" width=\"400\">\r\n</p>\r\n\r\n```\r\njava -Xmx512m -cp \"../lib/*\" -javaagent:../lib/sizeofag-1.0.0.jar \r\nmoa.DoTask EvaluatePrequential -l \\(bayes.AmidstClassifier -g 1 \r\n-m 2\\) -s generators.RandomRBFGenerator -i 10000 -f 1000 -q 1000\r\n```\r\n[[Back to Top]](#documentation)\r\n\r\n### AMIDST Regression from MOA <a name=\"moalinkregressionsexample\"></a>\r\n\r\nIt is possible to learn an enriched naive Bayes model for regression if the class label is of a continuous nature. The following command uses the model in Figure \\ref{fig:HODE}(b) on a toy dataset from WEKA's collection of [regression problems](http://prdownloads.sourceforge.net/weka/datasets-numeric.jar).\r\n\r\n\r\n<p align=\"center\">\r\n<img title=\"Taxonomy\" src=\"http://amidst.github.io/toolbox/images/regressionHODE.jpg\" width=\"400\">\r\n</p>\r\n\r\n\r\n```\r\njava -Xmx512m -cp \"../lib/*\" -javaagent:../lib/sizeofag-1.0.0.jar \r\nmoa.DoTask EvaluatePrequentialRegression -l bayes.AmidstRegressor\r\n -s (ArffFileStream -f ./quake.arff)\r\n```\r\n\r\nNote that the simpler the dataset the less complex the model should be. In this case, \\texttt{quake.arff} is a very simple and small dataset that should probably be learn with a more simple classifier, that is, a high-bias-low-variance classifier, in order to avoid overfitting. This aims at providing a simple running example.\r\n\r\n[[Back to Top]](#documentation)\r\n\r\n## Citing AMIDST Toolbox <a name=\"cite\"></a>\r\n\r\n\r\n[[Back to Top]](#documentation)","google":"UA-66233470-1","note":"Don't delete this file! It's used internally to help with page regeneration."}